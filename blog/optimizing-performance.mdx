---
title: What Happens when you Profiler a Profiler
sidebar_label: Optimizing Performance of Pyroscope
slug: /optimizing-performance
---

## Overview

Recently we released a new feature that we call pull mode. It allows you to get profiling data from applications and it has various discovery mechanisms so that you can easily integrate with things like kubernetes and start profiling all of your pods with minimum setup. This was an often requested feature as a lot of people are already using prometheus where pull mode is the default.

![image describing the difference between push mode and pull mode](https://user-images.githubusercontent.com/23323466/148311629-942596e5-73f0-419d-a07e-3345908c56dc.png)

Early on we had a user who ran pyroscope in pull mode with about a thousand of profiling targets and about 1 TB of raw profiling data per day.

In push mode, we've seen pyroscope handle similar amounts of traffic without issues. This is because in push mode pprof transcoding was done on each profiling target, and so the load was distributed across the many targets.

In pull mode, however, this transcoding logic was moved to the server, and thus no longer distributed across many targets. So, we immediately suspected that this performance regression might have something to do with the load increase on the pulling server.

## Using profiling to spot performance issues in Go

Pyroscope server is written in Golang and it continuously profiles itself. So, when these kinds of issues happen we're usually able to quickly find them.

<div style={{ width: "100%", margin: "auto" }}>

![screenshot of pyroscope showing the performance issue](https://user-images.githubusercontent.com/662636/149874240-438e017e-3c99-457f-963b-9d91d2a586ec.png)

</div>
<center style={{margin:"-15px 20px 20px"}}><i>screenshot of pyroscope showing the performance issue</i></center>

On this screenshot you can see that the 2 functions that take up a lot of time are `FindFunction` and `FindLocation`. Pyroscope transcodes each pprof profile into an internal flamegraph format and these functions are called as part of that process.

In order to understand why they're taking a lot of time let's see what a pprof object looks like:

<div style={{ width: "100%", margin: "auto", background: "rgba(255,255,255,0.33)", borderRadius: "10px" }}>


![anatomy of a pprof profile](https://user-images.githubusercontent.com/23323466/147887879-84058c51-5be4-4ccc-8c37-6784ddfc577d.png)

</div>
<center style={{margin:"-15px 20px 20px"}}><i>anatomy of a pprof profile</i></center>

Note that `location` and `function` fields are actually arrays, containing the locations and functions respectively. The objects in these arrays are identified by `ID`s.

`FindFunction` and `FindLocation` functions are almost identical and they both go over their respective arrays searching for objects by `ID`s.

```go
func FindFunction(x *Profile, fid uint64) (*Function, bool) {
	// this sort.Search function is the expensive part
	idx := sort.Search(len(x.Function), func(i int) bool {
		return x.Function[i].Id >= fid
	})

	if idx < len(x.Function) {
		if f := x.Function[idx]; f.Id == fid {
			return f, true
		}
	}
	return nil, false
}
```

And if you look closer at the functions they seem to be pretty optimized already — they use `sort.Search` which is a go implementation of [binary search algorithm](https://en.wikipedia.org/wiki/Binary_search_algorithm). We initially *assumed* that binary search would be the fastest here, because it's typically the fastest way to search for an element in a sorted array.

However, looking at the flamegraph, this was the bottleneck that was slowing down the whole system.

## Performance Improvement Attempt 1: When all else fails... Cache it!

In our first attempt at fixing the issue we tried to use caching. Instead of performing the binary search every time we needed to find a function, we cached the data in a hash map.

That did improve the performance a little bit, but really we just traded one relatively expensive operation (binary search) for another one (map lookups) that was slightly cheaper, but still expensive.

<div style={{ width: "100%", margin: "auto" }}>

![](https://user-images.githubusercontent.com/662636/149875102-d01f11cd-07a3-4f52-8c3a-3bc1c2d6e9e1.png)

</div>

<center style={{margin:"-15px 20px 20px"}}><i><code>tree.FindLocation</code> and <code>tree.FindFunction</code> are gone, but now we have <code>runtime.mapaccess2_fast64</code></i></center>

## Second attempt at solving the issue

As I mentioned earlier, objects in `function` and `location` arrays were sorted by `ID`. Upon closer inspection, we discovered that not only were the arrays sorted, but the `ID`s also started at 1 and ascended in consecutive numerical order (`1,2,3,4,5`). So if you wanted to get an object with ID of 10, you look at the object at position 9.

<!-- (TODO: ask Anton about pprof spec) -->

So, although we initially thought binary search was the fastest way to find `function`s and `location`s in their respective arrays, it turned out that we could eliminate the need to search altogether by referencing objects by their `ID – 1`.

This resulted in complete removal of the performance overhead caused by `FindFunction` and `FindLocation` functions.

<div style={{ width: "100%", margin: "auto" }}>

![screenshot of pyroscope showing that the issue was fixed](https://user-images.githubusercontent.com/80059/147078837-e711026d-0309-40fb-a32b-c24a52d31a55.png)

</div>

<center style={{margin:"-15px 20px 20px"}}><i>TODO: explain what happened</i></center>

## Continuous Benchmarking

One technique we're using at Pyroscope is what we call continuous benchmarking. As part of the continuous integration process, we run a benchmark on each commit. It runs pyroscope server with some synthesized load and this allows us to catch any performance regressions as well as test assumptions when it comes to performance improvements.

As the benchmark runs it collects various metrics about the performance of the server. These metrics provide a glance view of what's going on with the performance for each particular PR.

For example, for the improvement discussed in this blog post the benchmark showed the following results:

![screenshot of PR message](https://user-images.githubusercontent.com/662636/147839175-732c7a8f-eabd-4b4e-97eb-c40693be5b8e.png)
<center style={{margin:"-15px 20px 20px"}}><i>screenshot of PR message</i></center>

If you want to implement something like this for your own project, I welcome you to check out [the source code](https://github.com/pyroscope-io/pyroscope/tree/main/benchmark) of the benchmark suite.

Currently we have the whole suite implemented in one docker-compose file with configuration happening via environment variables. Here's a diagram of different components involved in the process:

![diagram of the continuous benchmarking integration](https://pyroscope-dima.s3.amazonaws.com/uploads/2021-12-31-13-24-36-BQQFLIKZFXLISIEW.svg)
<center style={{margin:"-15px 20px 20px"}}><i>diagram of the continuous benchmarking integration</i></center>

## Conclusion

In this blog post we've showed you how to methodically address performance issues and also how to implement a practice of continuous benchmarking in your own projects.

## Links

* [Pull Request described here](https://github.com/pyroscope-io/pyroscope/pull/628)
* [Benchmark suite](https://github.com/pyroscope-io/pyroscope/tree/main/benchmark)
